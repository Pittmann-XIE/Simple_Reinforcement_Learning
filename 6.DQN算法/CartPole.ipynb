{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "from collections import namedtuple, deque \n",
    "import random\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x1ce9036a5e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "class ReplayMemeory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity) # create an empty list with the length of capacity\n",
    "    \n",
    "    def push(self, *args): ## args accepts a tuple or list.\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__() ## initalize the nn.Modul using the DQN\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128,128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = F.relu(self.layer1(x))\n",
    "        out2 = F.relu(self.layer2(out1))\n",
    "        return self.layer3(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 # the number of transitions sampled from the replay buffer\n",
    "GAMMA = 0.99 # the discount fasctor as mentioned in the previous section\n",
    "EPS_START = 0.9 # the starting value of epsilon\n",
    "EPS_END = 0.05 # the final of the epsilon\n",
    "EPS_DECAY = 1e3 # controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "TAU = 5e-3 # the update rate of the target network\n",
    "LR = 1e-4 # leanring rate of the \"AdamW\" optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00234835 -0.03113144 -0.03141898 -0.01878157] {}\n"
     ]
    }
   ],
   "source": [
    "n_actions = env.action_space.n #the length of the action\n",
    "state, info = env.reset() # reset the game and get the state and info of the game\n",
    "print(state, info)\n",
    "n_observations = len(state) # the length of the state\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr = LR, amsgrad=True)\n",
    "memory = ReplayMemeory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_done = 0\n",
    "def select_action(state):\n",
    "    global step_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START-EPS_END)*(math.exp(-1.*step_done/EPS_DECAY))\n",
    "    step_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1,1) # transform the tensor into the shape (1,1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = [] # it stores the duration of the each episode\n",
    "\n",
    "def plot_duration(show_result=False):\n",
    "    plt.figure(1) # plot on the figure 1\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title(\"Result\")\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(\"Traning...\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Duration\")\n",
    "    plt.plot(durations_t.numpy()) # ocnvert durations_t from tensor to a numpy array\n",
    "    if len(durations_t) >= 100: # compute the moving average of the 100 episodes\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).reshape(-1) # var.unfold(dimension, size, step) can be used to create a moving window\n",
    "        # mean(dimension) computes the mean across a specific dimension\n",
    "        means = torch.cat((torch.zeros(99), means)) # to align it with the episode durations\n",
    "        plt.plot(means.numpy())\n",
    "    \n",
    "    plt.pause(1e-3)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf()) # get current figure\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions)) # batch would be in the form: [tensor1, None, tensor2, None, tensor3,...]\n",
    "\n",
    "    # create a boolean mask indicating which entries are not not \"None\". \"None\" usually mean the terminal state of the agent\n",
    "    # if the entry in batch.next_state = None, it will return \"False\" in this position\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), device=device, dtype=torch.bool) \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # compute the Q(s_t, a). the model computes Q(s_t), then we select the columns of the actions taken.\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch) # gather(dim, index)\n",
    "\n",
    "    # compute V(s_{t+1}) for all next states\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad(): # \n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(-1).values # only update the state which is not termial state\n",
    "    \n",
    "    expected_action_values = (next_state_values*GAMMA) + reward_batch\n",
    "\n",
    "    # compute the Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_action_values)\n",
    "\n",
    "    # optimize the model\n",
    "    optimizer.zero_grad() #delete the gradients of the previous loop\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100) # clip the model-parameters within 100 to avoid explosion\n",
    "    optimizer.step() # update the model using the computed parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_epsisodes = 600\n",
    "else:\n",
    "    num_epsisodes = 50\n",
    "\n",
    "for i_episode in range(num_epsisodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "        \n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(t+1)\n",
    "            plot_durations()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptot2_3_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
